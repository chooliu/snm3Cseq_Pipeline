{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # A07_compile_final_metadata overall cmds ===========================================\n",
    "\n",
    "# qsub Scripts/A07a_parse_metadata.sub # ‡\n",
    "# qsub Scripts/A07b_compile_metadata.sub # ‡\n",
    "\n",
    "# # ‡ fast enough to run interactively\n",
    "\n",
    "# # for interactive mode, just need to specify working dir & two environment variables\n",
    "# # then can run python code instead of \n",
    "# import os\n",
    "# os.chdir('../') # move to $dir_proj\n",
    "# os.environ['metadat_well'] = \"Metadata/A01c_well_filepath.csv\"\n",
    "# os.environ['ref_chromsizes'] = \"/u/project/cluo/chliu/Genomes/IGVF_hg38_pluslambda/chromsizes.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > ../Scripts/A07a_parse_metadata.sub\n",
    "\n",
    "#!/bin/bash\n",
    "#$ -cwd\n",
    "#$ -o sublogs/A07a_parse_metadata.$JOB_ID.$TASK_ID\n",
    "#$ -j y\n",
    "#$ -l h_rt=2:00:00,h_data=8G\n",
    "#$ -N A07a_parse_metadata\n",
    "#$ -t 1-7\n",
    "\n",
    "\n",
    "\n",
    "echo \"Job $JOB_ID.$SGE_TASK_ID started on:   \" `hostname -s`\n",
    "echo \"Job $JOB_ID.$SGE_TASK_ID started on:   \" `date `\n",
    "echo \" \"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# environment init -------------------------------------------------------------\n",
    "\n",
    ". /u/local/Modules/default/init/modules.sh # <--\n",
    "module load anaconda3 # <--\n",
    "conda activate snm3Cseq_taurus # <--\n",
    "\n",
    "export $(cat snm3C_parameters.env | grep -v '^#' | xargs) # <--\n",
    "\n",
    "\n",
    "\n",
    "# run each helper script (A07a*) ------------------------------------------------\n",
    "\n",
    "# note: in practice these can each be submitted interactively/as its own task,\n",
    "# as some of these scripts should be much lower resource than others\n",
    "# however, this -t 1-7 job parallelization is just for tidyness\n",
    "\n",
    "echo \"metadata script # $SGE_TASK_ID running:\"\n",
    "\n",
    "case $SGE_TASK_ID in\n",
    "\n",
    "  1)\n",
    "    echo \"python Scripts/A07a1_trimming.py\"\n",
    "    python Scripts/A07a1_trimming.py\n",
    "    ;;\n",
    "\n",
    "  2)\n",
    "    echo \"python Scripts/A07a2_mapping_rate.py\"\n",
    "    python Scripts/A07a2_mapping_rate.py\n",
    "    ;;\n",
    "\n",
    "  3)\n",
    "    echo \"python Scripts/A07a3_dedupe.py\"\n",
    "    python Scripts/A07a3_dedupe.py\n",
    "    ;;\n",
    "\n",
    "  4)\n",
    "    echo \"python Scripts/A07a4_global_mC_fracs.py\"\n",
    "    python Scripts/A07a4_global_mC_fracs.py\n",
    "    ;;\n",
    "\n",
    "  5)\n",
    "    echo \"python Scripts/A07a5_samtools_stats.py\"\n",
    "    python Scripts/A07a5_samtools_stats.py\n",
    "    ;;\n",
    "\n",
    "  6)\n",
    "    echo \"python Scripts/A07a6_coverage.py\"\n",
    "    python Scripts/A07a6_coverage.py\n",
    "    ;;\n",
    "\n",
    "  7)\n",
    "    echo \"python Scripts/A07a7_contacts.py\"\n",
    "    python Scripts/A07a7_contacts.py\n",
    "    ;;\n",
    "    \n",
    "  *)\n",
    "    ;;\n",
    "esac\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "echo \"completed 'A07a_parse_metadata.'\"\n",
    "\n",
    "\n",
    "\n",
    "echo \"Job $JOB_ID.$SGE_TASK_ID ended on:   \" `hostname -s`\n",
    "echo \"Job $JOB_ID.$SGE_TASK_ID ended on:   \" `date `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > ../Scripts/A07b_compile_metadata.sub\n",
    "\n",
    "#!/bin/bash\n",
    "#$ -cwd\n",
    "#$ -o sublogs/A07b_compile_metadata.$JOB_ID\n",
    "#$ -j y\n",
    "#$ -N A07b_compile_metadata\n",
    "#$ -l h_rt=0:30:00,h_data=4G\n",
    "#$ -hold_jid A07a_parse_metadata\n",
    "\n",
    "\n",
    "\n",
    "echo \"Job $JOB_ID started on:   \" `hostname -s`\n",
    "echo \"Job $JOB_ID started on:   \" `date `\n",
    "echo \" \"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# environment init -----------------------------------------------------------\n",
    "\n",
    ". /u/local/Modules/default/init/modules.sh # <--\n",
    "module load anaconda3 # <--\n",
    "conda activate snm3Cseq_taurus # <--\n",
    "\n",
    "export $(cat snm3C_parameters.env | grep -v '^#' | xargs) # <--\n",
    "\n",
    "\n",
    "\n",
    "# run metadat scripts --------------------------------------------------------\n",
    "\n",
    "python Scripts/A07b_compile_metadata.py\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "echo -e \"\\n\\n'A07b_compile_metadata' completed.\\n\\n\"\n",
    "\n",
    "\n",
    "\n",
    "echo \"Job $JOB_ID ended on:   \" `hostname -s`\n",
    "echo \"Job $JOB_ID ended on:   \" `date `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > ../Scripts/A07a1_trimming.py\n",
    "\n",
    "# A07a1_trimming.py ============================================================\n",
    "\n",
    "# setup ------------------------------------------------------------------------\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "filepath_wellmetadat = os.environ['metadat_well']\n",
    "metadata_well = pd.read_csv(filepath_wellmetadat)\n",
    "\n",
    "def parse_fastp_report(filepath):\n",
    "    jsonfile = pd.read_json(filepath)\n",
    "    dict_out = {\n",
    "        'nreads_pretrim' : jsonfile['summary']['before_filtering']['total_reads'],\n",
    "        'percreads_passtrim' : jsonfile['summary']['after_filtering']['total_reads'] /\n",
    "              jsonfile['summary']['before_filtering']['total_reads'],\n",
    "        'q20_pretrim' : jsonfile['summary']['before_filtering']['q30_rate'],\n",
    "        'q20_posttrim' : jsonfile['summary']['after_filtering']['q30_rate'],\n",
    "        'r1_len' : jsonfile['summary']['after_filtering']['read1_mean_length'],\n",
    "        'r2_len' : jsonfile['summary']['after_filtering']['read2_mean_length'],\n",
    "        'gc_perc' : jsonfile['summary']['after_filtering']['gc_content']}\n",
    "    return(dict_out)\n",
    "\n",
    "\n",
    "\n",
    "# gather metadata --------------------------------------------------------------\n",
    "\n",
    "filelist=metadata_well['A03a_json_fastp']\n",
    "boolean_fileexists = [os.path.exists(f) for f in filelist]\n",
    "list_fastp = [parse_fastp_report(f) for f in filelist[boolean_fileexists]]\n",
    "df_fastp = pd.DataFrame(list_fastp,\n",
    "                        index=metadata_well['wellprefix'][boolean_fileexists])\n",
    "\n",
    "# print percent files missing\n",
    "print(\"number of target files: \" + str(len(filelist)))\n",
    "print(\"fraction files missing: \")\n",
    "print(round(1 - sum(boolean_fileexists)/len(boolean_fileexists), 3))\n",
    "\n",
    "# column QC\n",
    "print(\"Number of NAs per column:\")\n",
    "print(df_fastp.isna().sum().to_string())\n",
    "\n",
    "print(\"\\nNumber of duplicated wells:\")\n",
    "ndupe=df_fastp.index.duplicated().sum()\n",
    "print(ndupe)\n",
    "\n",
    "\n",
    "\n",
    "# final export\n",
    "df_fastp.to_csv(\"Metadata/A07a1_trimming.tsv\", sep='\\t')\n",
    "print(\"done.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > ../Scripts/A07a2_mapping_rate.py\n",
    "\n",
    "# A07a2_mapping_rate.py ========================================================\n",
    "\n",
    "# setup ------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "filepath_wellmetadat = os.environ['metadat_well']\n",
    "metadata_well = pd.read_csv(filepath_wellmetadat)\n",
    "    \n",
    "def parse_bismark_report(filepath):\n",
    "\n",
    "    \"\"\"\n",
    "    parse bismark.txt output\n",
    "    adapted from YAP @ https://github.com/lhqing/cemba_data to include PE & SE output\n",
    "    commented out term_dict lines of limited interest\n",
    "    note that paired-end metrics usually yield fragments, versus reads\n",
    "    \"\"\"\n",
    "\n",
    "    term_dict = {\n",
    "        'Sequence pairs analysed in total': 'TotalReadPairsIn',\n",
    "        'Sequences analysed in total': 'TotalReadsIn',\n",
    "        'Number of paired-end alignments with a unique best hit': 'UniqueMappedPairs',\n",
    "        'Number of alignments with a unique best hit from the different alignments': 'UniqueMappedReads',\n",
    "        'Mapping efficiency': 'MappingRate',\n",
    "        # # other potential metrics, not usually used\n",
    "#         'Sequence pairs with no alignments under any condition': 'UnmappedPairs',\n",
    "#         'Sequences with no alignments under any condition': 'UnmappedReads',\n",
    "#         'Sequences did not map uniquely': 'AmbigReads',\n",
    "#         'Sequence pairs did not map uniquely': 'AmbigPairs',\n",
    "#         'CT/GA/CT': 'ReadsOT',\n",
    "#         'GA/CT/CT': 'ReadsOB',\n",
    "#         'GA/CT/GA': 'ReadsCTOT',\n",
    "#         'CT/GA/GA': 'ReadsCTOB',\n",
    "#         'CT/CT': 'ReadsOT',\n",
    "#         'CT/GA': 'ReadsOB',\n",
    "#         'GA/CT': 'ReadsCTOT',\n",
    "#         'GA/GA': 'ReadsCTOB',\n",
    "#         'Total number of C\\'s analysed': 'TotalC',\n",
    "#         'C methylated in CpG context': 'BismarkmCGRate',\n",
    "#         'C methylated in CHG context': 'BismarkmCHGRate',\n",
    "#         'C methylated in CHH context': 'BismarkmCHHRate',\n",
    "#         'C methylated in unknown context (CN or CHN)' : 'BismarkmCNCHNRate',\n",
    "#         'C methylated in Unknown context (CN or CHN)' : 'BismarkmCNCHNRate'\n",
    "        }\n",
    "\n",
    "    with open(filepath) as report:\n",
    "        report_dict = {}\n",
    "        for line in report:\n",
    "            try:\n",
    "                lhs, rhs = line.split(':')\n",
    "            except ValueError:\n",
    "                continue\n",
    "            try:\n",
    "                report_dict[term_dict[lhs]] = rhs.strip().split('\\t')[0].strip('%')\n",
    "            except KeyError:\n",
    "                pass\n",
    "            \n",
    "    return(report_dict)\n",
    "\n",
    "\n",
    "\n",
    "# prep file lists --------------------------------------------------------------\n",
    "\n",
    "# extract bismark logs that exist\n",
    "target_filepath_columns = metadata_well.columns[\n",
    "    metadata_well.columns.str.contains(\"A04a_bismarktxt_\")]\n",
    "filelist = metadata_well.loc[:, target_filepath_columns].values.tolist()\n",
    "filelist = [f for sublist in filelist for f in sublist]\n",
    "boolean_fileexists = [os.path.exists(f) for f in filelist]\n",
    "\n",
    "# print percent files missing\n",
    "print(\"number of target files: \" + str(len(filelist)))\n",
    "print(\"fraction files missing: \")\n",
    "print(round(1 - sum(boolean_fileexists)/len(boolean_fileexists), 3))\n",
    "\n",
    "# well list\n",
    "# anticipate 10 unique logs per well (2 full length, up to 6 splits)\n",
    "welllist = []\n",
    "for w in metadata_well.wellprefix:\n",
    "    welllist.extend([w]*10)\n",
    "\n",
    "# loop through files\n",
    "print(\"reading bismark logs...\")\n",
    "list_bismarkmap = [parse_bismark_report(f) for f in pd.Series(filelist)[boolean_fileexists]]\n",
    "\n",
    "df_mapping_byalign = pd.DataFrame(list_bismarkmap).apply(pd.to_numeric)\n",
    "df_mapping_byalign.index=pd.Series(welllist)[boolean_fileexists]\n",
    "\n",
    "# pairing metadata by alignment type\n",
    "\n",
    "print(\"pairing metadata with split/alignment type...\")\n",
    "df_mapping_byalign['Alignment'] = pd.Series(\n",
    "    target_filepath_columns.str.replace(\"A04a_bismarktxt_\", \"\").to_list() \\\n",
    "    * metadata_well.shape[0])[boolean_fileexists].to_list()\n",
    "\n",
    "alignnames_presplit = [\"R1p\", \"R2p\", \"R1trims\", \"R2trims\"]\n",
    "\n",
    "# can optionally save this detailed breakdown by alignment source\n",
    "# df_mapping_byalign.to_csv(\"Metadata/A07a2_mappingrate_detailed.tsv\", sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "# final metadata out -----------------------------------------------------------\n",
    "# join the TAURUS pre-split (full R1 and R2 mapping), then post-split\n",
    "\n",
    "print(\"now grouping by wellprefix...\")\n",
    "\n",
    "df_final = pd.DataFrame(index = metadata_well.wellprefix)\n",
    "\n",
    "df_presplit = \\\n",
    "    df_mapping_byalign[df_mapping_byalign['Alignment'].isin(alignnames_presplit)\n",
    "                      ].reset_index().groupby('index').agg('sum')\n",
    "df_final['NumReadsIn'] = df_presplit.TotalReadsIn\n",
    "df_final['UniqueMappedReads_PreSplit'] = df_presplit.UniqueMappedReads\n",
    "df_final['MappingRate_PreSplit'] = df_presplit.UniqueMappedReads/df_presplit.TotalReadsIn\n",
    "\n",
    "df_postsplit = \\\n",
    "    df_mapping_byalign[~df_mapping_byalign['Alignment'].isin(alignnames_presplit)\n",
    "                      ].reset_index().groupby('index').agg('sum')\n",
    "\n",
    "df_final['NumReadsIn_PostSplit'] = df_postsplit.TotalReadsIn\n",
    "df_final['UniqueMappedReads_PostSplit'] = df_postsplit.UniqueMappedReads\n",
    "df_final['MappingRate_PostSplit'] =  df_postsplit.UniqueMappedReads/df_postsplit.TotalReadsIn\n",
    "\n",
    "# final combined mapping rates\n",
    "# Nmappre + Nmappost/Nsplits; Nsplits = (Ninpost/Nunmapped) ~ 2-3 per read\n",
    "\n",
    "print(\"doing final summary by well...\")\n",
    "\n",
    "df_final['Alignments_Total_SplitAdj'] = \\\n",
    "    df_final.UniqueMappedReads_PreSplit + \\\n",
    "    df_final.UniqueMappedReads_PostSplit / \\\n",
    "    (df_final.NumReadsIn_PostSplit/(df_final.MappingRate_PreSplit*df_final.NumReadsIn))\n",
    "df_final['MappingRate_Total'] = \\\n",
    "    df_final.Alignments_Total_SplitAdj / df_final.NumReadsIn\n",
    "\n",
    "# column QC\n",
    "print(\"Number of NAs per column:\")\n",
    "print(df_final.isna().sum().to_string())\n",
    "\n",
    "print(\"\\nNumber of duplicated wells:\")\n",
    "ndupe=df_final.index.duplicated().sum()\n",
    "print(ndupe)\n",
    "\n",
    "\n",
    "\n",
    "# final export\n",
    "df_final.to_csv(\"Metadata/A07a2_mappingrate.tsv\", sep='\\t')\n",
    "print(\"done.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > ../Scripts/A07a3_dedupe.py\n",
    "\n",
    "# A07a3_dedupe.py ==============================================================\n",
    "\n",
    "# setup ------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "filepath_wellmetadat = os.environ['metadat_well']\n",
    "metadata_well = pd.read_csv(filepath_wellmetadat)\n",
    "\n",
    "nulltable = np.array([pd.NA, pd.NA, pd.NA]) \n",
    "\n",
    "def parse_dedupe(filepath):\n",
    "    try:\n",
    "        data_dedupe = pd.read_csv(filepath, delimiter = \"\\t\",\n",
    "                         comment = \"#\", nrows = 1)[[\n",
    "                             'UNPAIRED_READS_EXAMINED', 'READ_PAIRS_EXAMINED', 'PERCENT_DUPLICATION'\n",
    "                         ]].transpose()[0]\n",
    "        return(data_dedupe)\n",
    "    except:\n",
    "        return(nulltable)\n",
    "\n",
    "tidy_name_dict = {'PERCENT_DUPLICATION' : 'picard_perc_dupe',\n",
    "                  'READ_PAIRS_EXAMINED' : 'picard_npairsin',\n",
    "                  'UNPAIRED_READS_EXAMINED' : 'picard_nreadsin'}\n",
    "\n",
    "\n",
    "\n",
    "# gather metadata --------------------------------------------------------------\n",
    "\n",
    "filelist=metadata_well['A04a_log_picard']\n",
    "boolean_fileexists = [os.path.exists(f) for f in filelist]\n",
    "list_picard = [parse_dedupe(f) for f in filelist[boolean_fileexists]]\n",
    "\n",
    "df_picard = pd.DataFrame(list_picard,\n",
    "                            index = metadata_well['wellprefix'][boolean_fileexists]\n",
    "                           ).rename(columns = tidy_name_dict\n",
    "                           ).drop(\"picard_npairsin\", axis = 1)\n",
    "\n",
    "\n",
    "# print percent files missing\n",
    "print(\"number of target files: \" + str(len(filelist)))\n",
    "print(\"fraction files missing: \")\n",
    "print(round(1 - sum(boolean_fileexists)/len(boolean_fileexists), 3))\n",
    "\n",
    "# column QC\n",
    "print(\"Number of NAs per column:\")\n",
    "print(df_picard.isna().sum().to_string())\n",
    "\n",
    "print(\"\\nNumber of duplicated wells:\")\n",
    "ndupe=df_picard.index.duplicated().sum()\n",
    "print(ndupe)\n",
    "\n",
    "\n",
    "\n",
    "# final export\n",
    "df_picard.to_csv(\"Metadata/A07a3_dedupe.tsv\", sep='\\t')\n",
    "print(\"done.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > ../Scripts/A07a4_global_mC_fracs.py\n",
    "\n",
    "# A07a4_global_mC_fracs.py =====================================================\n",
    "\n",
    "# setup ------------------------------------------------------------------------\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "filepath_wellmetadat = os.environ['metadat_well']\n",
    "metadata_well = pd.read_csv(filepath_wellmetadat)\n",
    "\n",
    "attempt_dedupe = True # <-- attempt index repair?\n",
    "\n",
    "# gather metadata --------------------------------------------------------------\n",
    "\n",
    "filelist=[ \"Metadata/A04d_mCfrac_\" + str(batch_num) + \".tsv\"\n",
    "           for batch_num in pd.unique(metadata_well['batchnum']) ]\n",
    "boolean_fileexists = [os.path.exists(f) for f in filelist]\n",
    "\n",
    "list_mCfracs = [ pd.read_csv(f, delimiter = \"\\t\") for f in pd.Series(filelist)[boolean_fileexists]] \n",
    "df_mCfracs = pd.concat(list_mCfracs)\n",
    "df_mCfracs = df_mCfracs.rename(columns = {\"Well\" : \"wellprefix\"})\n",
    "\n",
    "\n",
    "\n",
    "# print number files\n",
    "print(\"number of target files: \" + str(len(filelist)))\n",
    "\n",
    "# column QC\n",
    "print(\"Number of NAs per column:\")\n",
    "print(df_mCfracs.isna().sum().to_string())\n",
    "\n",
    "print(\"\\nNumber of duplicated wells:\")\n",
    "ndupe=df_mCfracs.wellprefix.duplicated().sum()\n",
    "print(ndupe)\n",
    "\n",
    "\n",
    "\n",
    "# check for dupe wells ---------------------------------------------------------\n",
    "# the A04d scripts can generate duplicated wells due to the \"append\" option\n",
    "# or early terminated jobs which may result in NAs\n",
    "# thus we can sort by number of CHs observed in desc order --> remove first dupes\n",
    "if attempt_dedupe and ndupe != 0:\n",
    "    print(\"attempting to dedupe...\")\n",
    "    df_mCfracs = df_mCfracs.sort_values('CH')\n",
    "    df_mCfracs = df_mCfracs[~df_mCfracs.wellprefix.duplicated(keep = 'first')]\n",
    "\n",
    "\n",
    "\n",
    "# final export\n",
    "df_mCfracs = df_mCfracs.sort_values('wellprefix').reset_index(drop=True)\n",
    "df_mCfracs.to_csv(\"Metadata/A07a4_global_mC_fracs.tsv\", sep='\\t', index = False)\n",
    "print(\"done.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > ../Scripts/A07a5_samtools_stats.py\n",
    "\n",
    "# A07a5_samtools_stats.py ======================================================\n",
    "\n",
    "# setup ------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "filepath_wellmetadat = os.environ['metadat_well']\n",
    "metadata_well = pd.read_csv(filepath_wellmetadat)\n",
    "\n",
    "def parse_samstats(filepath):\n",
    "\n",
    "    term_dict = {\n",
    "        'raw total sequences': 'FilteredSeqCount',\n",
    "        'bases mapped' : 'BasesMapped',\n",
    "        'error rate': 'ErrorRate'\n",
    "        }\n",
    "\n",
    "    with open(filepath) as report:\n",
    "        report_dict = {}\n",
    "        for line in report:\n",
    "            try:\n",
    "                lhs, rhs = line.split(':')\n",
    "            except ValueError:\n",
    "                continue\n",
    "            try:\n",
    "                report_dict[term_dict[lhs]] = rhs.strip().split('\\t')[0]\n",
    "            except KeyError:\n",
    "                pass\n",
    "            \n",
    "    return(report_dict)\n",
    "\n",
    "\n",
    "\n",
    "# gather metadata --------------------------------------------------------------\n",
    "\n",
    "metadata_well = pd.read_csv(filepath_wellmetadat)\n",
    "\n",
    "filelist = metadata_well['A04c_txt_samstats']\n",
    "boolean_fileexists = [os.path.exists(f) for f in filelist]\n",
    "list_samstats = [parse_samstats(f) for f in filelist[boolean_fileexists]]\n",
    "df_samstats = pd.DataFrame(list_samstats,\n",
    "                        index=metadata_well['wellprefix'][boolean_fileexists])\n",
    "\n",
    "\n",
    "\n",
    "# print percent files missing\n",
    "print(\"number of target files: \" + str(len(filelist)))\n",
    "print(\"percent files missing: \")\n",
    "print(round(1 - sum(boolean_fileexists)/len(boolean_fileexists), 3))\n",
    "\n",
    "# column QC\n",
    "print(\"Number of NAs per column:\")\n",
    "print(df_samstats.isna().sum().to_string())\n",
    "\n",
    "print(\"\\nNumber of duplicated wells:\")\n",
    "ndupe=df_samstats.index.duplicated().sum()\n",
    "print(ndupe)\n",
    "\n",
    "\n",
    "\n",
    "# final export\n",
    "df_samstats.to_csv(\"Metadata/A07a5_samstats.tsv\", sep='\\t')\n",
    "print(\"done.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > ../Scripts/A07a6_coverage.py\n",
    "\n",
    "# A07a6_coverage.py ============================================================\n",
    "# setup ------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "filepath_wellmetadat = os.environ['metadat_well']\n",
    "metadata_well = pd.read_csv(filepath_wellmetadat)\n",
    "\n",
    "\n",
    "\n",
    "# extract autosomal ------------------------------------------------------------\n",
    "\n",
    "target_chroms = [\"chr\" + str(i) for i in range(1, 99)]\n",
    "autosomal_chroms = \\\n",
    "    pd.read_csv(os.environ['ref_chromsizes'],\n",
    "                sep = \"\\t\", header = None, index_col = 0)\n",
    "autosomal_chroms = autosomal_chroms[autosomal_chroms.index.isin(target_chroms)]\n",
    "total_autosomal_bases = autosomal_chroms.sum()\n",
    "target_chroms=autosomal_chroms.index\n",
    "\n",
    "\n",
    "\n",
    "# gather metadata: base-lvl unique coverage levels -----------------------------\n",
    "\n",
    "print(\"processing autosomal num sites with at least 1-fold coverage.\")\n",
    "print(\"if any filenames printed below, potentially corrupt files:\")\n",
    "def parse_coverage_unique(filepath):\n",
    "    try:\n",
    "        percent_coverage = \\\n",
    "            pd.read_csv(filepath, delimiter = \"\\s+\", header = None, index_col=1)\n",
    "        percent_coverage = (percent_coverage.loc[target_chroms, 0].sum() / total_autosomal_bases).to_list()[0]\n",
    "    except:\n",
    "        print(\"potentially corrupt file:\")\n",
    "        print(filepath)\n",
    "        percent_coverage = np.nan\n",
    "    return(percent_coverage)\n",
    "\n",
    "filelist=metadata_well['A04c_txt_covnsites']\n",
    "boolean_fileexists = [os.path.exists(f) for f in filelist]\n",
    "list_unique = [parse_coverage_unique(file) for file in filelist[boolean_fileexists]]\n",
    "df_unique = pd.DataFrame(list_unique,\n",
    "                        index = metadata_well['wellprefix'][boolean_fileexists])\n",
    "df_unique.columns = [\"CoveragePerc1x\"]\n",
    "\n",
    "\n",
    "\n",
    "# print percent files missing\n",
    "print(\"number of target files: \" + str(len(filelist)))\n",
    "print(\"fraction files missing: \")\n",
    "print(round(1 - sum(boolean_fileexists)/len(boolean_fileexists), 3))\n",
    "\n",
    "# column QC\n",
    "print(\"Number of NAs per column:\")\n",
    "print(df_unique.isna().sum().to_string())\n",
    "\n",
    "print(\"\\nNumber of duplicated wells:\")\n",
    "ndupe=df_unique.index.duplicated().sum()\n",
    "print(ndupe)\n",
    "\n",
    "\n",
    "\n",
    "df_unique.to_csv(\"Metadata/A07a6_DNA_cov_percent1x.tsv\", sep='\\t')\n",
    "\n",
    "print(\"done.\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# total coverage levels for chrX/chrY ------------------------------------------\n",
    "\n",
    "print(\"processing total coverage levels per chrom.\")\n",
    "print(\"if any filenames printed below, potentially corrupt files:\")\n",
    "def parse_coverage_total(filepath):\n",
    "    try:\n",
    "        total_cov_by_chr = pd.read_csv(filepath, delimiter = \"\\s+\", header = None, index_col=0)\n",
    "        if any(total_cov_by_chr.index==\"chrX\") and (not any(total_cov_by_chr.index==\"chrY\")):\n",
    "            coverage_XdivY = numpy.inf\n",
    "        else:\n",
    "            coverage_XdivY = total_cov_by_chr.loc['chrX', ] / total_cov_by_chr.loc['chrY', ]\n",
    "            coverage_XdivY = coverage_XdivY.tolist()[0]\n",
    "    except:\n",
    "        print(filepath)\n",
    "        coverage_XdivY = np.nan\n",
    "    return(coverage_XdivY)\n",
    "\n",
    "filelist=metadata_well['A04c_txt_covtot']\n",
    "boolean_fileexists = [os.path.exists(f) for f in filelist]\n",
    "list_total = [parse_coverage_total(file) for file in filelist[boolean_fileexists]]\n",
    "df_total = pd.DataFrame(list_total,\n",
    "             index = metadata_well['wellprefix'][boolean_fileexists])\n",
    "df_total.columns = [\"CoverageXdivY\"]\n",
    "\n",
    "\n",
    "\n",
    "# print percent files missing\n",
    "print(\"number of target files: \" + str(len(filelist)))\n",
    "print(\"fraction files missing: \")\n",
    "print(round(1 - sum(boolean_fileexists)/len(boolean_fileexists), 3))\n",
    "\n",
    "# column QC\n",
    "print(\"Number of NAs per column:\")\n",
    "print(df_total.isna().sum().to_string())\n",
    "\n",
    "print(\"\\nNumber of duplicated wells:\")\n",
    "ndupe=df_total.index.duplicated().sum()\n",
    "print(ndupe)\n",
    "\n",
    "\n",
    "\n",
    "# final export\n",
    "df_total.to_csv(\"Metadata/A07a6_DNA_cov_chrXdivY.tsv\", sep='\\t')\n",
    "print(\"done.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > ../Scripts/A07a7_contacts.py\n",
    "\n",
    "# A07a7_contacts.py ============================================================\n",
    "# setup ------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "filepath_wellmetadat = os.environ['metadat_well']\n",
    "metadata_well = pd.read_csv(filepath_wellmetadat)\n",
    "\n",
    "def parse_pairs(filepath, prefix = \"\"):\n",
    "    return(pd.read_csv(filepath, delimiter=\"\\t\"))\n",
    "\n",
    "\n",
    "\n",
    "# gather metadata --------------------------------------------------------------\n",
    "\n",
    "filelist=metadata_well['A06a_3c_metadat']\n",
    "boolean_fileexists = [os.path.exists(f) for f in filelist]\n",
    "list_contacts = [parse_pairs(file) for file in filelist[boolean_fileexists]]\n",
    "df_contacts = pd.concat(list_contacts).set_index(metadata_well['wellprefix'][boolean_fileexists])\n",
    "\n",
    "\n",
    "\n",
    "# print percent files missing\n",
    "print(\"number of target files: \" + str(len(filelist)))\n",
    "print(\"fraction files missing: \")\n",
    "print(round(1 - sum(boolean_fileexists)/len(boolean_fileexists), 3))\n",
    "\n",
    "# column QC\n",
    "print(\"Number of NAs per column:\")\n",
    "print(df_contacts.isna().sum().to_string())\n",
    "\n",
    "print(\"\\nNumber of duplicated wells:\")\n",
    "ndupe=df_contacts.index.duplicated().sum()\n",
    "print(ndupe)\n",
    "\n",
    "\n",
    "\n",
    "# final export\n",
    "df_contacts.to_csv(\"Metadata/A07a7_contact_metadat.tsv\", sep='\\t')\n",
    "print(\"done.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > ../Scripts/A07b_compile_metadata.py\n",
    "\n",
    "# A07b_compile_metadata.py =====================================================\n",
    "# assumes no changes to script output names from A07a*s\n",
    "\n",
    "# setup ------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "    \n",
    "\n",
    "\n",
    "# load tables ------------------------------------------------------------------\n",
    "# basic aggregation based on wellprefix (should be index of each A07a* output)\n",
    "\n",
    "def read_tbl_wrapper(filepath, prefix = \"\"):\n",
    "    return(pd.read_csv(filepath, delimiter = \"\\t\", index_col = 0).add_prefix(prefix))\n",
    "\n",
    "target_files = [\"Metadata/A07a1_trimming.tsv\",\n",
    "                \"Metadata/A07a2_mappingrate.tsv\",\n",
    "                \"Metadata/A07a3_dedupe.tsv\",\n",
    "                \"Metadata/A07a4_global_mC_fracs.tsv\",\n",
    "                \"Metadata/A07a5_samstats.tsv\",\n",
    "                \"Metadata/A07a6_DNA_cov_chrXdivY.tsv\", \"Metadata/A07a6_DNA_cov_percent1x.tsv\",\n",
    "                \"Metadata/A07a7_contact_metadat.tsv\"]\n",
    "                \n",
    "# Note: joining all tables may yield\n",
    "# \"InvalidIndexError: Reindexing only valid with uniquely valued Index objects\"\n",
    "# which indicates that there's a metadata file containing duplicated wells\n",
    "# this usually is a problem with script A07a4\n",
    "metadata_mC = pd.concat([read_tbl_wrapper(f) for f in target_files], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "# column QC\n",
    "print(\"Number of NAs per column:\")\n",
    "print(\"(note, these may vary from A07a* scripts b/c those skip missing files)\")\n",
    "print(metadata_mC.isna().sum().to_string())\n",
    "\n",
    "print(\"\\nNumber of duplicated wells:\")\n",
    "ndupe=metadata_mC.index.duplicated().sum()\n",
    "print(ndupe)\n",
    "\n",
    "\n",
    "\n",
    "# final export\n",
    "metadata_mC.to_csv(\"Metadata/A07b_compiled_metadata.tsv\", sep = \"\\t\")\n",
    "print(\"done.\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
