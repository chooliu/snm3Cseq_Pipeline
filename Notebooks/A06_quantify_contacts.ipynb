{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # A06_quantify_contacts overall cmds =========================================\n",
    "\n",
    "# qsub Scripts/A06a_quant_contacts.sub # †\n",
    "# qsub Scripts/A06b_check_contacts.sub\n",
    "\n",
    "# # * = job array based on \"platenum\"\n",
    "# # † = job array based on \"batchnum\" (two rows at a time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .bam to pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > ../Scripts/A06a_quant_contacts.sub\n",
    "\n",
    "#!/bin/bash\n",
    "#$ -cwd\n",
    "#$ -o sublogs/A06a_quant_contacts.$JOB_ID.$TASK_ID\n",
    "#$ -j y\n",
    "#$ -l h_rt=2:00:00,h_data=24G\n",
    "#$ -N A06a_quant_contacts\n",
    "#$ -t 1-512\n",
    "#$ -hold_jid_ad A04a_bismark\n",
    "\n",
    "\n",
    "\n",
    "echo \"Job $JOB_ID.$SGE_TASK_ID started on:   \" `hostname -s`\n",
    "echo \"Job $JOB_ID.$SGE_TASK_ID started on:   \" `date `\n",
    "echo \" \"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# environment init -------------------------------------------------------------\n",
    "\n",
    ". /u/local/Modules/default/init/modules.sh # <--\n",
    "module load anaconda3 # <--\n",
    "conda activate snm3Cseq_taurus # <--\n",
    "\n",
    "export $(cat snm3C_parameters.env | grep -v '^#' | xargs) # <--\n",
    "\n",
    "skip_complete=true # <-- for help with incomplete jobs\n",
    "\n",
    "\n",
    "\n",
    "# extract target filepaths -----------------------------------------------------\n",
    "\n",
    "# helper functions\n",
    "query_metadat () {\n",
    "  awk -F',' -v targetcol=\"$1\" \\\n",
    "      'NR==1 {\n",
    "                for (i=1;i<=NF;i++) {\n",
    "                    if ($i==targetcol) {assayout=i; break} }\n",
    "                print $assayout\n",
    "              }\n",
    "      NR>1 {\n",
    "                print $assayout\n",
    "            }' $metadat_well\n",
    "}\n",
    "\n",
    "# extract target wells, print values for log\n",
    "\n",
    "batchnum=($(query_metadat \"batchnum\"))\n",
    "nwells=${#batchnum[@]}\n",
    "\n",
    "target_well_rows=()\n",
    "for ((row=1; row<=nwells; row++))\n",
    "do\n",
    "    if [[ \"${batchnum[$row]}\" == \"$SGE_TASK_ID\" ]]\n",
    "    then\n",
    "        target_well_rows+=($row)\n",
    "    fi\n",
    "done\n",
    "\n",
    "\n",
    "\n",
    "# filepaths associated with target rows in well-level metadata -----------------\n",
    "\n",
    "wellprefix=($(query_metadat \"wellprefix\"))\n",
    "dir_well=($(query_metadat \"A04a_dir_bismark\"))\n",
    "\n",
    "bam_in=($(query_metadat \"A04a_bam_final\"))\n",
    "\n",
    "file_pairs=($(query_metadat \"A06a_pairs\"))\n",
    "file_meta3c=($(query_metadat \"A06a_3c_metadat\"))\n",
    "\n",
    "\n",
    "\n",
    "# print target files -----------------------------------------------------------\n",
    "\n",
    "echo \"batch number: $SGE_TASK_ID\"\n",
    "echo \"processing the following rows in well metadata file ($metadat_well):\"\n",
    "for row in ${target_well_rows[@]}\n",
    "    do\n",
    "        echo -e \"$row\\t${wellprefix[$row]}\"\n",
    "    done\n",
    "echo -e \"\\n\\n\"\n",
    "\n",
    "\n",
    "\n",
    "# contact-quant & metadat collection on each well in the batch -----------------\n",
    "\n",
    "for row in ${target_well_rows[@]} \n",
    "do\n",
    "\n",
    "    cd ${dir_proj}\n",
    "    \n",
    "    if [[ -s ${file_pairs[$row]} \\\n",
    "        && -s ${file_meta3c[$row]} ]]\n",
    "    then\n",
    "        echo -e \"pairs & metadat for '${wellprefix[$row]}' already exist.\"\n",
    "        \n",
    "        if [[ \"${skip_complete}\" == \"true\" ]]\n",
    "        then\n",
    "            echo \"skip_complete == true. skipping this well.'\"\n",
    "            continue\n",
    "        else\n",
    "            echo \"skip_complete != true. re-running this well.'\"\n",
    "        fi\n",
    "    fi\n",
    "\n",
    "    if [[ ! -s ${dir_proj}/${bam_in[$row]} ]]\n",
    "    then\n",
    "        echo \"input .bam file missing for '${wellprefix[$row]}'? skipping this well.\"\n",
    "        continue\n",
    "    fi\n",
    "    \n",
    "    echo -e \"\\n\\nprofiling contacts for '${wellprefix[$row]}'...\\n\\n\"\n",
    "    cd ${dir_well[$row]}\n",
    "    \n",
    "    # generates pairs.tsv and metadat_pairs.tsv\n",
    "    # 1-2 min / well --> <1 hr per batch expected\n",
    "    python ${dir_proj}/Scripts/A06a_quantify_contacts_TAURUS.py \"${dir_proj}/${bam_in[$row]}\"\n",
    "\n",
    "done\n",
    "\n",
    "\n",
    "\n",
    "echo -e \"\\n\\n'A06a_quant_contacts' completed.\\n\\n\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "echo \"Job $JOB_ID.$SGE_TASK_ID ended on:   \" `hostname -s`\n",
    "echo \"Job $JOB_ID.$SGE_TASK_ID ended on:   \" `date `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > ../Scripts/A06a_quantify_contacts_TAURUS.py\n",
    "\n",
    "# A06a_quantify_contacts_TAURUS.py, v0.1 =======================================\n",
    "# this is a re-code of TAURUS-MH by @chooliu \n",
    "# trades off some readability/modularity versus efficiency (reads in memory)\n",
    "# ==============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bamread\n",
    "\n",
    "\n",
    "\n",
    "# define valid chromosomes =====================================================\n",
    "\n",
    "# here just for for chromosome ordering / order of pairs in final output\n",
    "# select chromosomes to include in contact matrices\n",
    "\n",
    "chrom_sizes = \\\n",
    "    pd.read_csv(os.environ['ref_chromsizes'], sep=\"\\t\", header=None\n",
    "               ).set_axis(['chr', 'len'], axis = 1)\n",
    "chrom_sizes['chr'] = pd.Categorical(chrom_sizes['chr'],\n",
    "    categories = chrom_sizes['chr'] , ordered = True)\n",
    "\n",
    "# by default include anything in genome ref files...\n",
    "list_valid_chrom = chrom_sizes['chr']\n",
    "\n",
    "# # but some example alternatives below: could be all autosomal contacts, or +XYM\n",
    "# # caution: check how yr downstream HiC/3C software of choice deals with non-autosomal chr\n",
    "# list_valid_chrom = [\"chr\" + str(i) for i in range(1, 99)] + [\"chrX\", \"chrY\", \"chrM\"]\n",
    "# list_valid_chrom = [\"chr\" + str(i) for i in range(1, 99)] \n",
    "\n",
    "def tidy_chr_order(x):\n",
    "    return(pd.Categorical(x, categories = list_valid_chrom, ordered = True))\n",
    "\n",
    "\n",
    "\n",
    "# load alignments & process info per read name =================================\n",
    "# check format, fields, filtering (e.g., add MAPQ filt) if aligner changes\n",
    "\n",
    "alignments = bamread.read_bam_full(sys.argv[1])\n",
    "\n",
    "alignments = \\\n",
    "    alignments.drop([\"QueryStart\", \"Cigar\", \"Quality\", \"QuerySequence\"], axis = 1\n",
    "               ).rename({'QueryEnd': 'Length'}, axis = 1)\n",
    "\n",
    "# parse .fastq file names (including edits from 'split')\n",
    "# order of 'split' category represents preference when taking \"outer-most\" contacts\n",
    "alignments['ReadPrefix'] = alignments['Name'].str.split(\":N:0:\").str[0]\n",
    "alignments['Split'] = alignments['ReadPrefix'].str.split(\"_\").str[1] \n",
    "alignments['Split'] = pd.Categorical(\n",
    "    alignments['Split'],\n",
    "    categories=[\"1\", \"1:P1\", \"1:P2\", \"1:P3\", \"2:P3\", \"2:P2\", \"2:P1\", \"2\"],\n",
    "    ordered=True)\n",
    "alignments['ReadPrefix'] = alignments['ReadPrefix'].str.split(\"_\").str[0]\n",
    "alignments['ReadPair'] = alignments['Split'].str.split(\":\").str[0] # 1 or 2\n",
    "\n",
    "# 'pos' represents the 5' position of each read\n",
    "# depending on convention could also report \"leftmost\" (smallest val on REF) or \"rightmost\"\n",
    "alignments['Pos'] = alignments['Start'].where(\n",
    "    alignments['ReadPair']==\"1\", other=alignments['End'])\n",
    "\n",
    "# let a candidate contact = read names with more than\n",
    "# read/split alignment type -- (1, 2, 1:P1, ..., 2:P3)\n",
    "readprefix_candidate_contact = alignments.groupby(\"ReadPrefix\")['Split'].nunique()\n",
    "\n",
    "# distribution of splits aligned per readname\n",
    "# possible values are between 1 and 4\n",
    "n_align_per_read = readprefix_candidate_contact.value_counts().sort_index()\n",
    "\n",
    "# examine readprefixes with n >= 2 splits per readname\n",
    "# as they are candidates that contain possible intra- or intra-chromosomal info about contacts\n",
    "readprefix_candidate_contact = readprefix_candidate_contact[readprefix_candidate_contact >= 2].index\n",
    "\n",
    "# define potential contacts as \"outermost\" read 5'-positions a la TAURUS-MH\n",
    "# among readprefixes with multiple alignments\n",
    "# (only one contact possible per read pair)\n",
    "contacts_taurus = \\\n",
    "    alignments[alignments['ReadPrefix'].isin(readprefix_candidate_contact)\n",
    "              ].groupby(['ReadPrefix']\n",
    "              ).filter(lambda x : len(x) > 1\n",
    "              ).groupby(['ReadPrefix']).nth([0, -1]\n",
    "              ).reset_index()\n",
    "\n",
    "# for readprefixes with intrachr, sorts so that chr1 < chr3 first in df\n",
    "contacts_taurus = \\\n",
    "    contacts_taurus.assign(Chromosome=tidy_chr_order(contacts_taurus.loc[:, 'Chromosome'])\n",
    "                          ).sort_values(['ReadPrefix', 'Chromosome', 'Pos'])\n",
    "\n",
    "\n",
    "# note: may keep read name in future (to be true .pairs format)\n",
    "# would need to add new column with readname\n",
    "contacts_taurus = contacts_taurus.drop('ReadPrefix', axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "# TAURUS-like --> contacts file ================================================\n",
    "\n",
    "# initialize\n",
    "pairs_taurus = \\\n",
    "    pd.DataFrame(index = range(int((contacts_taurus.shape[0] + 1)/2)),\n",
    "                 columns = range(6))\n",
    "pairs_taurus.columns = ['chr1', 'pos1', 'chr2', 'pos2', 'strand1', 'strand2']\n",
    "\n",
    "# alignments --> pairs\n",
    "pairs_taurus.iloc[:, [0, 1]] = \\\n",
    "    contacts_taurus[['Chromosome', 'Pos']].iloc[0::2, ].reset_index(drop = True)\n",
    "pairs_taurus.iloc[:, [2, 3]] = \\\n",
    "    contacts_taurus[['Chromosome', 'Pos']].iloc[1::2, ].reset_index(drop = True)\n",
    "pairs_taurus.iloc[:, 4] = contacts_taurus['Strand'].iloc[0::2, ]\n",
    "pairs_taurus.iloc[:, 5] = contacts_taurus['Strand'].iloc[1::2, ]\n",
    "\n",
    "# for intra distances, calculate distance\n",
    "pair_is_intra = pairs_taurus['chr1'] == pairs_taurus['chr2']\n",
    "intra_dist = pairs_taurus['pos2'] - pairs_taurus['pos1']\n",
    "\n",
    "# optional: exclude close by contacts that may be re-ligations or non-contact (e.g., >1kb only)\n",
    "# note: there can small # pairs with intra dist exactly == 0 excluded here (short insert sizes)\n",
    "min_intra_dist = 0 # <-- optionally increase from default of 0\n",
    "filt_intra_dist = intra_dist > min_intra_dist\n",
    "\n",
    "# duplicated chr pos strand (both pairs)\n",
    "filt_nondupe = ~pairs_taurus.duplicated()\n",
    "\n",
    "# chromosome valid\n",
    "filt_chrom = pairs_taurus['chr1'].isin(list_valid_chrom) & \\\n",
    "    pairs_taurus['chr2'].isin(list_valid_chrom)\n",
    "\n",
    "\n",
    "\n",
    "# print out final =======================================================================\n",
    "\n",
    "# criteria for final filtering:\n",
    "# - either intra-chrom greater than minimum distance threshold, or inter-chrom\n",
    "# - within target chromosomes (\"list_valid_chrom\")\n",
    "# - non-duplicated by position\n",
    "filter_final = (~ pair_is_intra | filt_intra_dist ) & filt_chrom & filt_nondupe\n",
    "final_out = pairs_taurus[filter_final]\n",
    "\n",
    "final_out = final_out.assign(chr1=tidy_chr_order(final_out.loc[:, 'chr1'])\n",
    "                            ).assign(chr2=tidy_chr_order(final_out.loc[:, 'chr2']))\n",
    "final_out = final_out.sort_values(['chr1', 'chr2', 'pos1', 'pos2'])\n",
    "\n",
    "final_out.to_csv(\"pairs.tsv\", sep = \"\\t\", index_label=False, index = False, header=False)\n",
    "\n",
    "\n",
    "\n",
    "# final output stats =======================================================================\n",
    "\n",
    "dict_output_stats = {\n",
    "\n",
    "    # readnames\n",
    "    'readnames_anyalignment' : alignments['ReadPrefix'].nunique(),     # readnames with at least one alignment, in any split type (1, 1:P1, ..., 2:P3, 2)\n",
    "    'readnames_singlealign' : n_align_per_read.loc[1, ], # read names with only one alignment (non-informative)\n",
    "    \n",
    "    # alignments\n",
    "    'alignments_total' : alignments.shape[0],     # total number of alignments\n",
    "    'alignments_wholeread' : sum(alignments['Split'].isin(['1', '2'])),     # alignments based on 1 or 2\n",
    "    'alignments_taurus_3split' : sum(~alignments['Split'].isin(['1', '2'])), # alignments from 1:P1, ..., 2:P3\n",
    "\n",
    "    # contacts prefiltering\n",
    "    'contacts_prefilt' : len(readprefix_candidate_contact), # num candidate contacts pre-filt\n",
    "    'contacts_prefilt_minintralen' : sum(~(~ pair_is_intra | filt_intra_dist )), # inter or >min_intra_dist\n",
    "    'contacts_prefilt_nondupe' : sum(~filt_nondupe), # duplicated chr:pos:strand\n",
    "    'contacts_prefilt_validchrom' : sum(~filt_chrom), # \"list_valid_chrom\"\n",
    "    \n",
    "    # optional pre-filtering info --\n",
    "    # comment in below if want to check any bias induced by \"pre-filtering\"\n",
    "#     'contacts_prefilt_intra' : sum(pair_is_intra), # pair is on same chrom\n",
    "#     'contacts_prefilt_inter' : sum(~pair_is_intra), # pair on different chromosomes\n",
    "#     'contacts_prefilt_intra_less1000' : sum(pair_is_intra & (intra_dist <= 1000)), # same chromosome\n",
    "#     'contacts_prefilt_intra_greater1000' : sum(pair_is_intra & (intra_dist > 1000)), # same chrom, >1k\n",
    "#     'contacts_prefilt_intra_greater10000' : sum(pair_is_intra & (intra_dist > 10000)), # same chrom >10k\n",
    "    \n",
    "    # final contacts\n",
    "    'contacts_final' : sum(filter_final), # final number contacts\n",
    "    'contacts_final_intra' : sum(filter_final & pair_is_intra), # final, same chrom\n",
    "    'contacts_final_inter' : sum(filter_final & ~pair_is_intra), # final, diff chrom\n",
    "\n",
    "    'contacts_final_intra_less1000' : sum(filter_final & pair_is_intra & (intra_dist <= 1000)), # final, diff chrom\n",
    "    'contacts_final_intra_greater1000' : sum(filter_final & pair_is_intra & (intra_dist > 1000)), # final, diff chrom\n",
    "    'contacts_final_intra_greater10000' : sum(filter_final & pair_is_intra & (intra_dist > 10000))\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# fractions / ratios of values calculated above\n",
    "\n",
    "dict_output_stats['frac_readnames_yieldcontact'] = \\\n",
    "    dict_output_stats['contacts_final']/dict_output_stats['readnames_anyalignment']\n",
    "dict_output_stats['frac_contacts_passfilt'] =\\\n",
    "    dict_output_stats['contacts_final']/dict_output_stats['contacts_prefilt']\n",
    "    \n",
    "dict_output_stats['frac_finalcont_inter'] = \\\n",
    "    dict_output_stats['contacts_final_inter']/dict_output_stats['contacts_final']\n",
    "dict_output_stats['frac_finalcont_intraless1kb_totintra'] = \\\n",
    "    dict_output_stats['contacts_final_intra_less1000']/dict_output_stats['contacts_final_intra']\n",
    "dict_output_stats['frac_finalcont_intragreater1kb_totintra'] = \\\n",
    "    dict_output_stats['contacts_final_intra_greater1000']/dict_output_stats['contacts_final_intra']\n",
    "dict_output_stats['frac_finalcont_intragreater10kb_totintra'] = \\\n",
    "    dict_output_stats['contacts_final_intra_greater10000']/dict_output_stats['contacts_final_intra']\n",
    "    \n",
    "dict_output_stats['ratio_finalcont_intragreater1kb_inter'] = \\\n",
    "    dict_output_stats['contacts_final_intra_greater1000']/dict_output_stats['contacts_final_inter']\n",
    "\n",
    "\n",
    "# final output\n",
    "pd.DataFrame.from_dict(dict_output_stats, orient = \"index\"\n",
    "    ).transpose().to_csv(\"metadat_pairs.tsv\", sep = \"\\t\", index_label=False, index = False)\n",
    "    \n",
    "print(\"finished processing \" + sys.argv[1] + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > ../Scripts/A06b_check_contacts.sub\n",
    "\n",
    "#!/bin/bash\n",
    "#$ -cwd\n",
    "#$ -o sublogs/A06b_check_contacts.$JOB_ID\n",
    "#$ -j y\n",
    "#$ -l h_rt=2:00:00,h_data=4G\n",
    "#$ -N A06b_contactcheck\n",
    "#$ -hold_jid A06a_quant_contacts\n",
    "\n",
    "\n",
    "\n",
    "echo \"Job $JOB_ID started on:   \" `hostname -s`\n",
    "echo \"Job $JOB_ID started on:   \" `date `\n",
    "echo \" \"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# environment init -------------------------------------------------------------\n",
    "\n",
    "export $(cat snm3C_parameters.env | grep -v '^#' | xargs) # <--\n",
    "\n",
    "\n",
    "\n",
    "# extract target filepaths -----------------------------------------------------\n",
    "\n",
    "query_metadat () {\n",
    "  awk -F',' -v targetcol=\"$1\" \\\n",
    "      'NR==1 {\n",
    "                for (i=1;i<=NF;i++) {\n",
    "                    if ($i==targetcol) {assayout=i; break} }\n",
    "                print $assayout\n",
    "              } \n",
    "      NR>1 {\n",
    "                print $assayout\n",
    "            }' $metadat_well\n",
    "}\n",
    "\n",
    "check_filepaths_in_assay() {\n",
    "    for file in $@\n",
    "        do \n",
    "        if [[ ! -s $file ]]\n",
    "            then\n",
    "                echo \"missing '$file'\"\n",
    "            fi\n",
    "        done\n",
    "}\n",
    "\n",
    "check_filepath_by_batch() {\n",
    "target_array=($@)\n",
    "batches_to_rerun=()\n",
    "for ((target_batch=1; target_batch<=nbatches; target_batch++))\n",
    "    do\n",
    "        target_well_rows=()\n",
    "        for ((row=1; row<=nwells; row++))\n",
    "        do\n",
    "            if [[ \"${batchnum[$row]}\" == \"${target_batch}\" ]]\n",
    "            then\n",
    "                target_well_rows+=($row)\n",
    "            fi\n",
    "        done\n",
    "\n",
    "        batch_file_list=${target_array[@]: ${target_well_rows[0]}:${#target_well_rows[@]} }\n",
    "    \n",
    "        num_files_missing=$(check_filepaths_in_assay ${batch_file_list[@]} | wc -l)\n",
    "\n",
    "        if [[ ${num_files_missing} > 0 ]]\n",
    "        then\n",
    "            batches_to_rerun+=(${target_batch})\n",
    "            echo -e \"${target_batch} \\t ${num_files_missing}\"\n",
    "        fi\n",
    "    done \n",
    "    \n",
    "    if [[ ${#batches_to_rerun[@]} > 0 ]]\n",
    "    then\n",
    "        echo \"batches to re-run:\"\n",
    "        echo \"${batches_to_rerun[*]}\"        \n",
    "    fi\n",
    "}\n",
    "\n",
    "batchnum=($(query_metadat \"batchnum\"))\n",
    "\n",
    "nwells=${#batchnum[@]}\n",
    "nbatches=${batchnum[-1]}\n",
    "\n",
    "\n",
    "\n",
    "# apply checks for A04a output -------------------------------------------------\n",
    "\n",
    "echo \"-----------------------------------------------------------------\"\n",
    "echo \"A. printing number of final .pairs/metadat missing (by batch)... \"\n",
    "echo -e \"-----------------------------------------------------------------\\n\\n\"\n",
    "\n",
    "file_pairs=($(query_metadat \"A06a_pairs\"))\n",
    "file_meta3c=($(query_metadat \"A06a_3c_metadat\"))\n",
    "\n",
    "echo \"checking contact pairs:\"\n",
    "echo -e \"batchnum\\tnum_missing\"\n",
    "check_filepath_by_batch ${file_pairs[@]}\n",
    "\n",
    "echo \"checking pairs metadata:\"\n",
    "check_filepath_by_batch ${file_meta3c[@]}\n",
    "\n",
    "\n",
    "echo -e \"\\n\\nsuggest re-running and checking sublog output of above batches.\"\n",
    "\n",
    "\n",
    "\n",
    "echo -e \"\\n\\n-----------------------------------------------------------------\"\n",
    "echo \"B. checking each expected .pairs/metadat file (from $metadat_well)\"\n",
    "echo -e \"-----------------------------------------------------------------\\n\\n\"\n",
    "\n",
    "echo \"* checks the A06a output columns of 'metadat_well' if the file exists and is non-empty.\"\n",
    "echo \"* if none missing, will only output target column names above.\"\n",
    "echo \"* if some declared 'missing' but all other checks OK, may just be no/few reads surviving trimming.\"\n",
    "echo \"  (check 'fastq_demultip/' and associated fastp logs e.g., fastq_trimmed/wellprefix.html report)\"\n",
    "\n",
    "echo -e \"\\nchecking contact pairs:\\n\"\n",
    "check_filepaths_in_assay ${file_pairs[@]}\n",
    "\n",
    "echo -e \"\\nchecking pairs metadata:\\n\"\n",
    "check_filepaths_in_assay ${file_meta3c[@]}\n",
    "\n",
    "\n",
    "\n",
    "echo -e \"\\n\\n-----------------------------------------------------------------\"\n",
    "echo \"C. checking log files for issues.\"\n",
    "echo -e \"-----------------------------------------------------------------\\n\"\n",
    "\n",
    "echo \"checking if 'completed' in sublogs/A06a_quant_contacts* output.\"\n",
    "echo \"if any filename is printed, the associated batch may have not completed allc gen.\"\n",
    "\n",
    "grep -c 'ended on' sublogs/A06a_quant_contacts* | awk -F \":\" '$2==0 {print $1}'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "echo -e \"\\n\\n'A06b_check_contacts' completed.\\n\\n\"\n",
    "\n",
    "\n",
    "\n",
    "echo \"Job $JOB_ID ended on:   \" `hostname -s`\n",
    "echo \"Job $JOB_ID ended on:   \" `date `\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
