{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # A01_mergefastq_preptargets overall commands ================================\n",
    "\n",
    "# qsub Scripts/A01a_merge_lanes.sub # *\n",
    "# qsub Scripts/A01b_plate_metadata.sub # ‡\n",
    "\n",
    "# # * = job array based on \"platenum\"\n",
    "# # † = job array based on \"batchnum\" (two rows at a time)\n",
    "# # ‡ fast enough to run interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of .fastq R1+R2 files\n",
      "512\n",
      "\n",
      "\n",
      "\n",
      "example fastq names\n",
      "20231005-3C29D12-Pos1-B06_S16_L001_R1_001.fastq.gz\n",
      "20231005-3C29D12-Pos1-B06_S16_L001_R2_001.fastq.gz\n",
      "20231005-3C29D12-Pos1-B06_S16_L002_R1_001.fastq.gz\n",
      "20231005-3C29D12-Pos1-B06_S16_L002_R2_001.fastq.gz\n",
      "20231005-3C29D12-Pos1-B06_S16_L003_R1_001.fastq.gz\n",
      "20231005-3C29D12-Pos1-B06_S16_L003_R2_001.fastq.gz\n",
      "20231005-3C29D12-Pos1-B06_S16_L004_R1_001.fastq.gz\n",
      "20231005-3C29D12-Pos1-B06_S16_L004_R2_001.fastq.gz\n",
      "20231005-3C29D12-Pos1-B06_S16_L005_R1_001.fastq.gz\n",
      "20231005-3C29D12-Pos1-B06_S16_L005_R2_001.fastq.gz\n",
      "\n",
      "\n",
      "\n",
      "plate names (based on file name before lane 'L00*')\n",
      "check that these are unique -- will be used to merge across lane!\n",
      "      8 20231005-3C29D12-Pos1-B06_S16\n",
      "      8 20231005-3C29D12-Pos2-C03_S24\n",
      "      8 20231005-3C29D16-Pos1-B01_S11\n",
      "      8 20231005-3C29D16-Pos2-B10_S19\n",
      "      8 20231005-3C29D1-Pos1-C04_S25\n",
      "      8 20231005-3C29D1-Pos2-A02_S1\n",
      "      8 20231005-3C37D5-Pos1-C10_S31\n",
      "      8 20231005-3C37D5-Pos2-A08_S7\n",
      "      8 20231005-3C37D7-Pos1-C07_S28\n",
      "      8 20231005-3C37D7-Pos2-A05_S4\n",
      "      8 20231005-3C37D9-Pos1-A12_S10\n",
      "      8 20231005-3C37D9-Pos2-B09_S18\n",
      "      8 20231005-3C38D0-Pos1-B05_S15\n",
      "      8 20231005-3C38D0-Pos2-C02_S23\n",
      "      8 20231005-3C38D12-Pos1-C08_S29\n",
      "      8 20231005-3C38D12-Pos2-A06_S5\n",
      "      8 20231005-3C38D14-Pos1-A11_S9\n",
      "      8 20231005-3C38D14-Pos2-B08_S17\n",
      "      8 20231005-3C38D5-Pos1-B02_S12\n",
      "      8 20231005-3C38D5-Pos2-B11_S20\n",
      "      8 20231005-3C39D0-Pos1-C06_S27\n",
      "      8 20231005-3C39D0-Pos2-A04_S3\n",
      "      8 20231005-3C39D12-Pos1-C11_S32\n",
      "      8 20231005-3C39D12-Pos2-A09_S8\n",
      "      8 20231005-3C39D14-Pos1-B04_S14\n",
      "      8 20231005-3C39D14-Pos2-C01_S22\n",
      "      8 20231005-3C39D1-Pos1-B03_S13\n",
      "      8 20231005-3C39D1-Pos2-B12_S21\n",
      "      8 20231005-3C39D5-Pos1-C05_S26\n",
      "      8 20231005-3C39D5-Pos2-A03_S2\n",
      "      8 20231005-3C39D9-Pos1-C09_S30\n",
      "      8 20231005-3C39D9-Pos2-A07_S6\n",
      "\n",
      "\n",
      "\n",
      "number of plates\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# before proceeding, check naming convention of\n",
    "# the raw .fastq files in $dir_originalfastq: \n",
    "# example shown for a 32-plate experiment from the IGVF m3C dataset\n",
    "\n",
    "# 512 .fastq files --> 256 read pairs (R1 and R2)\n",
    "# 256 read pairs/8 lanes = 32 plates\n",
    "dir_originalfastq=/u/project/cluo/Shared_Datasets/source_fastq/yzcl47\n",
    "\n",
    "echo \"number of .fastq R1+R2 files\"\n",
    "ls ${dir_originalfastq}/*fastq.gz | wc -l\n",
    "echo -e \"\\n\\n\"\n",
    "\n",
    "# print .fastq.gz examples names\n",
    "echo \"example fastq names\"\n",
    "ls ${dir_originalfastq} | head\n",
    "echo -e \"\\n\\n\"\n",
    "\n",
    "# print unique plate names, number of lanes per plate\n",
    "# our lab's convention is date-project-platemetadata-plateindexid\n",
    "# (check that this final lane-merged file is unique for each plate!)\n",
    "echo \"plate names (based on file name before lane 'L00*')\"\n",
    "echo \"check that these are unique -- will be used to merge across lane!\"\n",
    "for fastqfile in ${dir_originalfastq}/*R1*.fastq.gz;\n",
    "do\n",
    "    echo $(basename ${fastqfile%_L00[1-8]_*});\n",
    "done | uniq -c\n",
    "echo -e \"\\n\\n\"\n",
    "\n",
    "# number of samples\n",
    "echo \"number of plates\"\n",
    "for fastqfile in ${dir_originalfastq}/*R1*.fastq.gz;\n",
    "do\n",
    "    echo $(basename ${fastqfile%_L00[1-8]_*});\n",
    "done | uniq | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A01a) merge .fastq.gz by lane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > ../Scripts/A01a_merge_lanes.sub\n",
    "\n",
    "#!/bin/bash\n",
    "#$ -cwd\n",
    "#$ -o sublogs/A01a_merge_lanes.$JOB_ID.$TASK_ID\n",
    "#$ -j y\n",
    "#$ -l h_rt=8:00:00,h_data=16G\n",
    "#$ -N A01a_merge_lanes\n",
    "#$ -t 1-32\n",
    "\n",
    "\n",
    "\n",
    "echo \"Job $JOB_ID.$SGE_TASK_ID started on:   \" `hostname -s`\n",
    "echo \"Job $JOB_ID.$SGE_TASK_ID started on:   \" `date `\n",
    "echo \" \"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# environment init -------------------------------------------------------------\n",
    "\n",
    ". /u/local/Modules/default/init/modules.sh # <--\n",
    "\n",
    "export $(cat snm3C_parameters.env | grep -v '^#' | xargs)  # <--\n",
    "\n",
    "\n",
    "\n",
    "# get list of plates, files ----------------------------------------------------\n",
    "\n",
    "if [[ ! -s fastq_raw ]]\n",
    "then\n",
    "    mkdir fastq_raw\n",
    "fi\n",
    "\n",
    "list_of_plates=(\n",
    "  $(for plateid in ${dir_originalfastq}/*R1*;\n",
    "    do\n",
    "    echo $(basename ${plateid%_L00[1-8]_*});\n",
    "    done | uniq | sort))\n",
    "target_plate=${list_of_plates[$SGE_TASK_ID - 1]}\n",
    "\n",
    "\n",
    "# print array task and plate name\n",
    "# make sure ${target_plate} is uniquely identifiable &\n",
    "# doesn't group more than the four lanes typically excepected\n",
    "echo -e \"\\n\\ntarget plate number (SGE_TASK_ID):\" $SGE_TASK_ID\n",
    "echo \"target plate prefix:\" ${target_plate}\n",
    "\n",
    "\n",
    "\n",
    "# merge R1, then R2 files across lanes -----------------------------------------\n",
    "\n",
    "filesin_r1=($(ls ${dir_originalfastq}/*${target_plate}*R1*fastq.gz))\n",
    "filesin_r2=($(ls ${dir_originalfastq}/*${target_plate}*R2*fastq.gz))\n",
    "\n",
    "echo -e \"\\n\\nmerging Read 1 files:\"\n",
    "for file in ${filesin_r1[@]}\n",
    "do \n",
    "    du -h $file\n",
    "done\n",
    "cat ${filesin_r1[@]} > fastq_raw/${target_plate}_R1.fastq.gz\n",
    "\n",
    "echo -e \"\\n\\nmerging Read 2 files:\"\n",
    "for file in ${filesin_r2[@]}\n",
    "do \n",
    "    du -h $file\n",
    "done\n",
    "cat ${filesin_r2[@]} > fastq_raw/${target_plate}_R2.fastq.gz\n",
    "\n",
    "\n",
    "\n",
    "# check output files -----------------------------------------------------------\n",
    "\n",
    "echo -e \"\\n\\nchecking output file sizes.\"\n",
    "du -h fastq_raw/${target_plate}*fastq.gz\n",
    "\n",
    "echo -e \"\\n\\n'A01a_merge_lanes' completed.\\n\\n\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "echo \"Job $JOB_ID.$SGE_TASK_ID ended on:   \" `hostname -s`\n",
    "echo \"Job $JOB_ID.$SGE_TASK_ID ended on:   \" `date `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A01b) parse plate metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > ../Scripts/A01b_plate_metadata.sub\n",
    "\n",
    "#!/bin/bash\n",
    "#$ -cwd\n",
    "#$ -o sublogs/A01b_plate_metadata.$JOB_ID\n",
    "#$ -j y\n",
    "#$ -N A01b_plate_metadata\n",
    "#$ -l h_rt=0:10:00,h_data=4G\n",
    "#$ -hold_jid A01a_merge_lanes\n",
    "\n",
    "\n",
    "echo \"Job $JOB_ID started on:   \" `hostname -s`\n",
    "echo \"Job $JOB_ID started on:   \" `date `\n",
    "echo \" \"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# environment init -------------------------------------------------------------\n",
    "\n",
    ". /u/local/Modules/default/init/modules.sh\n",
    "module load anaconda3 # <--\n",
    "conda activate snm3Cseq_taurus # <--\n",
    "\n",
    "export $(cat snm3C_parameters.env | grep -v '^#' | xargs) # <--\n",
    "\n",
    "\n",
    "\n",
    "# run metadata compilation -----------------------------------------------------\n",
    "\n",
    "# because the two scripts are so fast,\n",
    "# violating my .sub & .py paired tidy convention and just running both here\n",
    "# (suggest running these in interactive mode anyway)\n",
    "\n",
    "python Scripts/A01b_plate_metadata.py\n",
    "python Scripts/A01c_well_filepaths.py\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "echo -e \"\\n\\n'A01b_plate_metadata' completed.\\n\\n\"\n",
    "\n",
    "\n",
    "\n",
    "echo \"Job $JOB_ID ended on:   \" `hostname -s`\n",
    "echo \"Job $JOB_ID ended on:   \" `date `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > ../Scripts/A01b_plate_metadata.py\n",
    "\n",
    "# ==============================================================================\n",
    "# Scripts/A01b_plate_metadata.py\n",
    "# should parse list of lane-merged plates -->\n",
    "# extract plate-level metadata saved to $dir_proj/Metadata\n",
    "# ==============================================================================\n",
    "\n",
    "# recommend running interactively in python/Jupyter to check outputs,\n",
    "# the relevant metadata parameters very likely to change between studies\n",
    "\n",
    "\n",
    "# load packages ----------------------------------------------------------------\n",
    "\n",
    "import glob\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# # if running interactively, need to load some lines from snm3C_parameters.env\n",
    "# # or manually spec os.environ -- e.g., via os.environ['dir_proj'] = \"mydirectory\" or this below loop\n",
    "# # (check relative path of parameters.env file or change to absolute if below not working!)\n",
    "# envvar_needed = ['dir_proj', 'dir_originalfastq', 'metadat_plate']\n",
    "# try:\n",
    "#     os.environ['dir_proj']\n",
    "# except KeyError:\n",
    "#     envspec = pd.read_csv(\"../snm3C_parameters.env\", sep = \"=\", comment=\"#\", header = None\n",
    "#                ).set_axis(['varname', 'varpath'], axis = 1\n",
    "#                ).query('varname in @envvar_needed')\n",
    "#     for index, row in envspec.iterrows():\n",
    "#         os.environ[row[\"varname\"]] = row[\"varpath\"]\n",
    "# os.chdir(os.environ['dir_proj'])\n",
    "\n",
    "\n",
    "\n",
    "# check fastq.gz names ---------------------------------------------------------\n",
    "\n",
    "fastq_dir = os.environ['dir_originalfastq']\n",
    "filepaths_raw_fastq = glob.glob(fastq_dir + \"*fastq.gz\")\n",
    "print( filepaths_raw_fastq[0:4] )\n",
    "\n",
    "\n",
    "# data.frame of plate names ----------------------------------------------------\n",
    "\n",
    "# split before lane (L00[1-8]) to get unique plate names\n",
    "plates_df = pd.DataFrame(\n",
    "    {'plate' : pd.unique([filepath.split(\"/\")[-1].split(\"_L\")[0] for filepath in filepaths_raw_fastq])}\n",
    "    ).sort_values('plate').reset_index(drop = True)\n",
    "\n",
    "# study specific metadata: edit these! # <--\n",
    "# example presented here is for IGVF cell lines (e.g., 20231005-3C29D1-Pos1-C04_S25)\n",
    "# info in filenames separated by -, change accordingly; will throw errors if fewer fields than in example\n",
    "plates_df['dateseq'] = plates_df['plate'].transform(lambda platename: platename.split(\"-\")[0])\n",
    "plates_df['sample'] = plates_df['plate'].transform(lambda platename: platename.split(\"-\")[1])\n",
    "plates_df['sort'] = plates_df['plate'].transform(lambda platename: platename.split(\"-\")[2])\n",
    "plates_df['plateindex'] = plates_df['plate'].transform(lambda platename: platename.split(\"-\")[3])\n",
    "\n",
    "# separating by \"D\" and removing \"3\" prefix from sample ID\n",
    "# because in this example, IGVF sample identifiers are \"3\" (for 3C) + linename + \"D\" + timepoint\n",
    "plates_df['line'] = plates_df['sample'].transform(lambda platename: platename.split(\"D\")[0]\n",
    "                                                 ).str.replace(\"^3\", \"\", regex = True) \n",
    "plates_df['time'] = plates_df['sample'].transform(lambda platename: platename.split(\"D\")[1])\n",
    "\n",
    "# number each plate, \"platenum\" used for batch submission later on\n",
    "# platenum indexed by 1-Nplates for compatibility with SGE (can't qsub -t 0)\n",
    "plates_df['platenum'] = plates_df.index.astype(int) + 1\n",
    "plates_df.index = plates_df.index.astype(int) + 1\n",
    "\n",
    "# export to \"Metadata/A01b_plate_metadata.csv\" by default\n",
    "print( plates_df.head() )\n",
    "print ( plates_df.shape )\n",
    "plates_df.to_csv(os.environ['metadat_plate'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A01c) expand plate --> all 384 wells --> final \"targets\" file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > ../Scripts/A01c_well_filepaths.py\n",
    "\n",
    "# ==============================================================================\n",
    "# Scripts/A01c_well_filepaths.py\n",
    "# expands plate-level metadata (A01b) into well-level metadata\n",
    "# ==============================================================================\n",
    "\n",
    "# recommend running interactively in python/Jupyter to check outputs,\n",
    "# but shouldn't require any changes to defaults\n",
    "\n",
    "# load packages ----------------------------------------------------------------\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# # if running interactively, need to load some lines from snm3C_parameters.env\n",
    "# # or manually spec os.environ -- e.g., via os.environ['dir_proj'] = \"mydirectory\" or this below loop\n",
    "# # (check relative path of parameters.env file or change to absolute if below not working!)\n",
    "# envvar_needed = ['dir_proj', 'dir_originalfastq', 'metadat_plate', 'metadat_well']\n",
    "# try:\n",
    "#     os.environ['dir_proj']\n",
    "# except KeyError:\n",
    "#     envspec = pd.read_csv(\"../snm3C_parameters.env\", sep = \"=\", comment=\"#\", header = None\n",
    "#                ).set_axis(['varname', 'varpath'], axis = 1\n",
    "#                ).query('varname in @envvar_needed')\n",
    "#     for index, row in envspec.iterrows():\n",
    "#         os.environ[row[\"varname\"]] = row[\"varpath\"]\n",
    "# os.chdir(os.environ['dir_proj'])\n",
    "\n",
    "\n",
    "\n",
    "# expand A01b metadata by well -------------------------------------------------\n",
    "\n",
    "# load A01b\n",
    "plates_df = pd.read_csv(os.environ['metadat_plate'], index_col=0)\n",
    "\n",
    "# from pandas documentation\n",
    "def expand_grid(data_dict):\n",
    "    \"\"\"Create a dataframe from every combination of given values.\"\"\"\n",
    "    rows = itertools.product(*data_dict.values())\n",
    "    return pd.DataFrame.from_records(rows, columns=data_dict.keys())\n",
    "\n",
    "filepath_df = expand_grid({'plate': plates_df['plate'],\n",
    "    'row' : [chr(x) for x in range(65, 65+16)],\n",
    "    'col' : [str(x + 1) for x in range(24)]})\n",
    "filepath_df['well'] = filepath_df[['row', 'col']].agg(''.join, axis = 1)\n",
    "filepath_df['wellprefix'] = filepath_df['plate'] + \"_\" + filepath_df['well']\n",
    "\n",
    "filepath_df = pd.merge(filepath_df, plates_df, how = \"left\", on = \"plate\")\n",
    "\n",
    "\n",
    "\n",
    "# batch into sets of 24 for bismark mapping, contact calling, etc --------------\n",
    "# (by default, one row at a time, incremented by \"batchnum\")\n",
    "\n",
    "# - alternatively, could make smaller batches of wells (e.g., n = 5) for compute\n",
    "#   environments that favor many small jobs versus a few long jobs,\n",
    "# - or... two sets of batches e.g., filepath_df['batchnum_A04a_bismark']\n",
    "#   pulled by the sub scripts for the A04a script / really resource-intensive jobs only\n",
    "\n",
    "nwellstot = filepath_df.shape[0]\n",
    "filepath_df['batchnum'] =\\\n",
    "    pd.Series(range(0, np.ceil(nwellstot / wells_per_batch).astype(int))\n",
    "             ).repeat(wells_per_batch)[0:nwellstot].reset_index(drop = True) + 1\n",
    "\n",
    "print( \"number of total wells:\" )\n",
    "print( nwellstot )\n",
    "\n",
    "print( \"wells per 'batchnum':\" )\n",
    "print( wells_per_batch )\n",
    "\n",
    "filepath_df.index = filepath_df.index.astype(int) + 1\n",
    "\n",
    "def basename(pathin):\n",
    "    return(pathin.split(\"/\")[-1])\n",
    "\n",
    "print( \"number of plates:\" )\n",
    "print( \"Nplates: \" + str( filepath_df['platenum'].max() ) )\n",
    "\n",
    "print( \"number of batches:\" )\n",
    "print( \"Nbatches: \" + str( filepath_df['batchnum'].max() ) )\n",
    "\n",
    "\n",
    "\n",
    "# then extensive file paths for sections A02-A06 -------------------------------\n",
    "# (inelegant, but useful for file checking/compiling info)\n",
    "\n",
    "# A02: demultiplexing \n",
    "# all in dir: fastq_demultip/\n",
    "\n",
    "filepath_df['A02a_fqgz_demultip_R1'] = \"fastq_demultip/\" + filepath_df[['plate', 'well']].agg('_'.join, axis = 1) + \"_indexed_R1.fastq.gz\"\n",
    "filepath_df['A02a_fqgz_demultip_R2'] = \"fastq_demultip/\" + filepath_df[['plate', 'well']].agg('_'.join, axis = 1) + \"_indexed_R2.fastq.gz\"\n",
    "\n",
    "filepath_df['A02a_txt_summary1'] = \"fastq_demultip/\" + filepath_df['plate'] + \"_summary_1.txt\"\n",
    "filepath_df['A02a_txt_summary2'] = \"fastq_demultip/\" + filepath_df['plate'] + \"_summary_2.txt\"\n",
    "\n",
    "\n",
    "\n",
    "# A03: trimming ----------------------------------------------------------------\n",
    "# all in dir: fastq_trimmed/\n",
    "\n",
    "filepath_df['A03a_fqgz_paired_R1'] = \"fastq_trimmed/\" + filepath_df['wellprefix'] + \"_paired_R1.fastq.gz\"\n",
    "filepath_df['A03a_fqgz_paired_R2'] = \"fastq_trimmed/\" + filepath_df['wellprefix'] + \"_paired_R2.fastq.gz\"\n",
    "\n",
    "filepath_df['A03a_fqgz_singletrim_R1'] = \"fastq_trimmed/\" + filepath_df['wellprefix'] + \"_singletrim_R1.fastq.gz\"\n",
    "filepath_df['A03a_fqgz_singletrim_R2'] = \"fastq_trimmed/\" + filepath_df['wellprefix'] + \"_singletrim_R2.fastq.gz\"\n",
    "\n",
    "filepath_df['A03a_json_fastp'] = \"fastq_trimmed/\" + filepath_df['wellprefix'] + \".json\"\n",
    "\n",
    "\n",
    "# A04: bismark -----------------------------------------------------------------\n",
    "\n",
    "filepath_df['A04a_dir_bismark'] = \"mapping_bismark/\" + filepath_df['wellprefix'] + \"/\"\n",
    "\n",
    "# (i) taurus step 1 mapping outputs\n",
    "filepath_df['A04a_bam_R1p'] = \\\n",
    "    filepath_df['A04a_dir_bismark'] + filepath_df['A03a_fqgz_paired_R1'].apply(basename).str.replace(\".fastq.gz\", \"_bismark.bam\")\n",
    "filepath_df['A04a_bam_R2p'] = \\\n",
    "    filepath_df['A04a_dir_bismark'] + filepath_df['A03a_fqgz_paired_R2'].apply(basename).str.replace(\".fastq.gz\", \"_bismark.bam\")\n",
    "filepath_df['A04a_bam_R1trims'] = \\\n",
    "        filepath_df['A04a_dir_bismark'] + filepath_df['A03a_fqgz_singletrim_R1'].apply(basename).str.replace(\".fastq.gz\", \"_bismark.bam\")\n",
    "filepath_df['A04a_bam_R2trims'] = \\\n",
    "    filepath_df['A04a_dir_bismark'] + filepath_df['A03a_fqgz_singletrim_R2'].apply(basename).str.replace(\".fastq.gz\", \"_bismark.bam\")\n",
    "\n",
    "# step 1 logs\n",
    "filepath_df['A04a_bismarktxt_R1p'] = \\\n",
    "    filepath_df['A04a_dir_bismark'] + filepath_df['wellprefix'] + \"_paired_R1_bismark_SE_report.txt\"\n",
    "filepath_df['A04a_bismarktxt_R2p'] = \\\n",
    "    filepath_df['A04a_dir_bismark'] + filepath_df['wellprefix'] + \"_paired_R2_bismark_SE_report.txt\"\n",
    "filepath_df['A04a_bismarktxt_R1trims'] = \\\n",
    "        filepath_df['A04a_dir_bismark'] + filepath_df['wellprefix'] + \"_singletrim_R1_bismark_SE_report.txt\"\n",
    "filepath_df['A04a_bismarktxt_R2trims'] = \\\n",
    "    filepath_df['A04a_dir_bismark'] + filepath_df['wellprefix'] + \"_singletrim_R2_bismark_SE_report.txt\"\n",
    "\n",
    "# (ii) taurus step 2 logs\n",
    "filepath_df['A04a_bismarktxt_R1p1'] = \\\n",
    "    filepath_df['A04a_dir_bismark'] + \"subseq_R1_1_bismark_SE_report.txt\"\n",
    "filepath_df['A04a_bismarktxt_R1p2'] = \\\n",
    "    filepath_df['A04a_dir_bismark'] + \"subseq_R1_2_bismark_SE_report.txt\"\n",
    "filepath_df['A04a_bismarktxt_R1p3'] = \\\n",
    "    filepath_df['A04a_dir_bismark'] + \"subseq_R1_3_bismark_SE_report.txt\"\n",
    "filepath_df['A04a_bismarktxt_R2p1'] = \\\n",
    "    filepath_df['A04a_dir_bismark'] + \"subseq_R2_1_bismark_SE_report.txt\"\n",
    "filepath_df['A04a_bismarktxt_R2p2'] = \\\n",
    "    filepath_df['A04a_dir_bismark'] + \"subseq_R2_2_bismark_SE_report.txt\"\n",
    "filepath_df['A04a_bismarktxt_R2p3'] = \\\n",
    "    filepath_df['A04a_dir_bismark'] + \"subseq_R2_3_bismark_SE_report.txt\"\n",
    "\n",
    "# (iii) picard de-duplication\n",
    "filepath_df['A04a_log_picard'] = filepath_df['A04a_dir_bismark'] + \"picard.log\"\n",
    "\n",
    "# (iv) final merged, sorted, dedupe bam\n",
    "filepath_df['A04a_bam_final'] = filepath_df['A04a_dir_bismark'] + \"merged_dedupe.bam\"\n",
    "\n",
    "# A04c: mapping stats ----------------------------------------------------------\n",
    "\n",
    "filepath_df['A04c_txt_samstats'] = filepath_df['A04a_dir_bismark'] + \"samstats.txt\"\n",
    "filepath_df['A04c_txt_covtot'] = filepath_df['A04a_dir_bismark'] + \"nbases_cov_by_chr.txt\"\n",
    "filepath_df['A04c_txt_covnsites'] = filepath_df['A04a_dir_bismark'] + \"total_cov_by_chr.txt\"\n",
    "\n",
    "# A05a: methylation quantification ---------------------------------------------\n",
    "\n",
    "filepath_df['A05a_allc'] = filepath_df['A04a_dir_bismark'] + \"allc.tsv.gz\"\n",
    "filepath_df['A05a_allctbi'] = filepath_df['A04a_dir_bismark'] + \"allc.tsv.gz.tbi\"\n",
    "\n",
    "# A06: contact mapping ---------------------------------------------------------\n",
    "\n",
    "filepath_df['A06a_pairs'] = filepath_df['A04a_dir_bismark'] + \"pairs.tsv.gz\"\n",
    "filepath_df['A06a_3c_metadat'] = filepath_df['A04a_dir_bismark'] + \"metadat_pairs.tsv\"\n",
    "\n",
    "\n",
    "\n",
    "# finally, export --------------------------------------------------------------\n",
    "# by default exports to Metadata/A01c_well_filepath.csv\n",
    "\n",
    "\n",
    "print(\"final metadata file dimensions:\")\n",
    "print(filepath_df.shape)\n",
    "filepath_df.to_csv(os.environ['metadat_well'])\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
